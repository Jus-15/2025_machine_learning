# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x-q7XxWkTR_iUIILk3889ZaxPbaNErfU
"""

# 有使用gpt
import math
import numpy as np
import pandas as pd
from pathlib import Path
from xml.etree import ElementTree as ET
import matplotlib.pyplot as plt

from google.colab import files # 在 Colab 上傳 XML(資料檔)
print("請上傳 O-A0038-003.xml …")
uploaded = files.upload()    # 選檔上傳

# 題目給的參數
LON0 = 120.00    # 左下角經度
LAT0 = 21.88    # 左下角緯度
DLON = 0.03     # 經度解析度
DLAT = 0.03     # 緯度解析度
NX  = 67      # 經度個數
NY  = 120     # 緯度個數
NPTS = NX * NY  # 總資料個數 8040
INVALID_VALUE = -999.0     # 無效值

def _try_parse_number_list(text: str):
    # 目標 : 從檔案抓出長度為8040的一串浮點數
    cand = text.replace(",", " ").split() # 將逗號轉成空白
    try:
        vals = list(map(float, cand))
        if len(vals) == NPTS: # 若資料個數是題目給的8040則回傳
            return vals
    except Exception:
        pass

#讀檔函數
def extract_grid_values_from_xml(xml_path: str):
    root = ET.parse(xml_path).getroot() # 掃描整個檔案且用樹來儲存資料，並給出根節點
    full = ET.tostring(root, encoding="unicode") # 將樹轉成字串
    # 掃描每個節點
    for elem in root.iter():
        for candidate in (elem.text, elem.tail):
            if candidate:
                v = _try_parse_number_list(candidate)
                if v is not None:
                    return np.array(v, dtype=float)
    raise ValueError(f"檔案中找不到剛好 {NPTS} 個數字的資料區塊")


# 建立經緯度網格
def build_lon_lat_grid():
    lons_1d = LON0 + np.arange(NX) * DLON
    lats_1d = LAT0 + np.arange(NY) * DLAT
    lon_grid, lat_grid = np.meshgrid(lons_1d, lats_1d)
    return lon_grid, lat_grid


def make_datasets(vals_1d: np.ndarray):
    arr = vals_1d.reshape(NY, NX)  # (120,67)
    lon_grid, lat_grid = build_lon_lat_grid()

    # 分類
    labels = (arr != INVALID_VALUE).astype(int)
    df_cls = pd.DataFrame({
        "lon": lon_grid.ravel(),
        "lat": lat_grid.ravel(),
        "label": labels.ravel()
    })

    # 回歸（只保留有效的）
    mask = (arr != INVALID_VALUE)
    df_reg = pd.DataFrame({
        "lon": lon_grid[mask],
        "lat": lat_grid[mask],
        "value": arr[mask]
    })
    return df_cls, df_reg


# 規範化(min-max scaling)
def scale_lonlat_to_m1p1(lon: np.ndarray, lat: np.ndarray):
    lon_min, lon_max = LON0, LON0 + (NX-1)*DLON
    lat_min, lat_max = LAT0, LAT0 + (NY-1)*DLAT
    lon_s = 2.0*(lon - lon_min)/(lon_max - lon_min) - 1.0
    lat_s = 2.0*(lat - lat_min)/(lat_max - lat_min) - 1.0
    return lon_s, lat_s


# 使用稍微修改過的上次作業的神經網路(四層，兩個隱藏層神經元皆為10個)
def sigmoid(x):
    return 1.0/(1.0 + np.exp(-x))

def Weight_Initialization(m, n, generator):
    limit = 5.0
    W = generator.uniform(-limit, limit, size=(m, n))
    b = generator.uniform(-limit, limit, size=(m, 1))
    return W, b

def forward_pass(x, params, task="reg"):
    W2, b2 = params["W2"], params["b2"]
    W3, b3 = params["W3"], params["b3"]
    W4, b4 = params["W4"], params["b4"]
    z2 = W2 @ x + b2; a2 = sigmoid(z2)
    z3 = W3 @ a2 + b3; a3 = sigmoid(z3)
    z4 = W4 @ a3 + b4
    y_hat = sigmoid(z4) if task=="clf" else z4 # 分類模型的輸出會再加上一個sigmoid函數(使其輸出在0到1之間)
    return y_hat, (a2, a3)

def train_twohidden_mlp(
    X_tr, y_tr,
    X_val=None, y_val=None,
    task="reg", in_dim=2, H1=10, H2=10, out_dim=1,
    epochs=100, batch_size=16, lr=0.05,
    pos_weight=None
):
    rng = np.random.default_rng()
    W2, b2 = Weight_Initialization(H1, in_dim, rng)
    W3, b3 = Weight_Initialization(H2, H1, rng)
    W4, b4 = Weight_Initialization(out_dim, H2, rng)
    params = {"W2": W2, "b2": b2, "W3": W3, "b3": b3, "W4": W4, "b4": b4}

    def mse_loss(yh, y): return np.mean((yh - y)**2)
    def bce_loss(yh, y, eps=1e-9, pw = pos_weight): # 參考 CSDN 的 BCE Loss
        yh = np.clip(yh, eps, 1.0-eps) # 避免 log(0) 出現
        return -np.mean( pw * y * np.log(yh) + (1-y) * np.log(1-yh))
    if task == "clf":
        if pos_weight is None:
            y_flat = y_tr.ravel()
            pos = float((y_flat == 1).sum())
            neg = float((y_flat == 0).sum())
            pos_weight = (neg / pos) if pos > 0 else 1.0
        # 保險（防止遇到如 pos=0 的極端情形）
        if not np.isfinite(pos_weight):
            pos_weight = 1.0

    hist_tr, hist_va = [], []
    N = X_tr.shape[1]

    for ep in range(1, epochs+1):
        idx = rng.permutation(N)
        total = 0.0
        for i in range(0, N, batch_size):
            ib = idx[i:i+batch_size]
            xb = X_tr[:, ib]      # (in_dim, B)
            yb = y_tr[:, ib]      # (out_dim, B)
            B  = xb.shape[1]

            yh, (a2,a3) = forward_pass(xb, params, task=task)
            if task=="reg":
                dL_dy = 2.0*(yh - yb)/B
                dL_dz4 = dL_dy
                total += mse_loss(yh, yb)*B
            else:
                eps = 1e-9
                yh_c = np.clip(yh, eps, 1.0-eps)
                total += bce_loss(yh_c, yb, pw=pos_weight)*B
                dL_dyh = -(pos_weight * yb / yh_c) + ((1.0 - yb) / (1.0 - yh_c))
                dL_dz4 = dL_dyh * (yh_c * (1.0 - yh_c))

            dL_dW4 = dL_dz4 @ a3.T
            dL_db4 = np.sum(dL_dz4, axis=1, keepdims=True)

            dL_da3 = params["W4"].T @ dL_dz4
            dL_dz3 = dL_da3 * (a3 * (1.0 - a3))
            dL_dW3 = dL_dz3 @ a2.T
            dL_db3 = np.sum(dL_dz3, axis=1, keepdims=True)

            dL_da2 = params["W3"].T @ dL_dz3
            dL_dz2 = dL_da2 * (a2 * (1.0 - a2))
            dL_dW2 = dL_dz2 @ xb.T
            dL_db2 = np.sum(dL_dz2, axis=1, keepdims=True)

            params["W4"] -= lr * dL_dW4; params["b4"] -= lr * dL_db4
            params["W3"] -= lr * dL_dW3; params["b3"] -= lr * dL_db3
            params["W2"] -= lr * dL_dW2; params["b2"] -= lr * dL_db2

        tr_loss = total/N
        hist_tr.append(tr_loss)
        if X_val is not None and y_val is not None:
            yh_v, _ = forward_pass(X_val, params, task=task)
            va_loss = mse_loss(yh_v, y_val) if task=="reg" else bce_loss(yh_v, y_val, pw=pos_weight)
        else:
            va_loss = np.nan
        hist_va.append(va_loss)

        if ep % 10 == 0: # 每十個印一次結果
            if task=="reg":
                rmse = np.sqrt(np.mean((yh_v - y_val)**2)) if X_val is not None else float('nan')
                print(f"[回歸 ep {ep:03d}] train_mse={tr_loss:.4f}  val_mse={va_loss:.4f}  val_rmse={rmse:.3f}")
            else:
                acc = None
                if X_val is not None:
                    acc = ((yh_v>=0.5).astype(int)==y_val).mean()
                print(f"[分類 ep {ep:03d}] train_bce={tr_loss:.4f}  val_bce={va_loss:.4f}  val_acc={acc if acc is not None else float('nan'):.4f}")
    return params, {"train_loss": hist_tr, "val_loss": hist_va}


# 讀檔
xml_path = "O-A0038-003.xml"
print(f"讀取 XML：{xml_path}")

vals_1d = extract_grid_values_from_xml(xml_path)
print("建立資料集 …")
df_cls, df_reg = make_datasets(vals_1d)

# 輸出 CSV
df_cls.to_csv("classification_dataset.csv", index=False, encoding="utf-8")
df_reg.to_csv("regression_dataset.csv",   index=False, encoding="utf-8")
print("已輸出：classification_dataset.csv")
print("已輸出：regression_dataset.csv")
print(f"分類資料集大小：{df_cls.shape}（含無效/有效點）")
print(f"回歸資料集大小：{df_reg.shape}（僅有效點）")


# 準備分類模型要用到的資料並縮放到 [-1,1]
lon_s, lat_s = scale_lonlat_to_m1p1(df_cls["lon"].values, df_cls["lat"].values)
Xc = np.vstack([lon_s, lat_s])         # (2, N)
yc = df_cls["label"].values.reshape(1, -1)   # (1, N)

# 打亂資料順序
rng = np.random.default_rng()
N = Xc.shape[1]
perm = rng.permutation(N)

# 訓練集資料量 : 驗證集 : 測試集 = 6 : 2 : 2
n_tr = int(0.6 * N); n_va = int(0.2 * N)
id_tr, id_va, id_te = perm[:n_tr], perm[n_tr:n_tr+n_va], perm[n_tr+n_va:]
Xtr_c, Xva_c, Xte_c = Xc[:, id_tr], Xc[:, id_va], Xc[:, id_te]
ytr_c, yva_c, yte_c = yc[:, id_tr], yc[:, id_va], yc[:, id_te]

# 訓練分類模型
params_c, hist_c = train_twohidden_mlp(
    X_tr=Xtr_c, y_tr=ytr_c, X_val=Xva_c, y_val=yva_c,
    task="clf", in_dim=2, H1=10, H2=10, out_dim=1,
    epochs=300, batch_size=16, lr=0.01
)
yte_prob, _ = forward_pass(Xte_c, params_c, task="clf")
yte_pred = (yte_prob >= 0.5).astype(int)
acc_te = (yte_pred == yte_c).mean()
tn = int(((yte_c==0)&(yte_pred==0)).sum())
fp = int(((yte_c==0)&(yte_pred==1)).sum())
fn = int(((yte_c==1)&(yte_pred==0)).sum())
tp = int(((yte_c==1)&(yte_pred==1)).sum())
cm = np.array([[tn, fp],[fn, tp]])
print("\n=== 分類模型 ===")
print(f"Test Accuracy: {acc_te:.4f}")
print("Confusion Matrix:\n", cm)

# 準備回歸模型要用到的資料並縮放到 [-1,1]
lon_sr, lat_sr = scale_lonlat_to_m1p1(df_reg["lon"].values, df_reg["lat"].values)
Xr = np.vstack([lon_sr, lat_sr])          # (2, Nr)
yr = df_reg["value"].values.reshape(1, -1)     # (1, Nr)

Nr = Xr.shape[1]
perm_r = rng.permutation(Nr)
ntr_r = int(0.6*Nr); nva_r = int(0.2*Nr)
id_tr_r, id_va_r, id_te_r = perm_r[:ntr_r], perm_r[ntr_r:ntr_r+nva_r], perm_r[ntr_r+nva_r:]
Xtr_r, Xva_r, Xte_r = Xr[:, id_tr_r], Xr[:, id_va_r], Xr[:, id_te_r]
ytr_r, yva_r, yte_r = yr[:, id_tr_r], yr[:, id_va_r], yr[:, id_te_r]

# 訓練回歸模型
params_r, hist_r = train_twohidden_mlp(
    X_tr=Xtr_r, y_tr=ytr_r, X_val=Xva_r, y_val=yva_r,
    task="reg", in_dim=2, H1=10, H2=10, out_dim=1,
    epochs=800, batch_size=4, lr=0.005
)
yte_hat, _ = forward_pass(Xte_r, params_r, task="reg")
rmse = float(np.sqrt(np.mean((yte_hat - yte_r)**2)))
r2  = float(1.0 - np.sum((yte_hat - yte_r)**2)/np.sum((yte_r - yte_r.mean())**2))
print("\n=== 回歸模型 ===")
print(f"Test RMSE: {rmse:.3f}")
print(f"Test R^2 : {r2:.3f}")

# 分類損失曲線
plt.figure(figsize=(7,5))
plt.plot(range(1, len(hist_c["train_loss"])+1), hist_c["train_loss"], label="Train Loss")
plt.plot(range(1, len(hist_c["val_loss"])+1),   hist_c["val_loss"],   label="Val Loss")
plt.title("Classification - Loss Curve")
plt.xlabel("Epoch"); plt.ylabel("Loss (BCE)")
plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()
plt.show()

# 回歸損失曲線
plt.figure(figsize=(7,5))
plt.plot(range(1, len(hist_r["train_loss"])+1), hist_r["train_loss"], label="Train Loss")
plt.plot(range(1, len(hist_r["val_loss"])+1),   hist_r["val_loss"],   label="Val Loss")
plt.title("Regression - Loss Curve")
plt.xlabel("Epoch"); plt.ylabel("Loss (MSE)")
plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()
plt.show()

# 分類：驗證/測試機率分佈
plt.figure(figsize=(6,4))
yva_prob, _ = forward_pass(Xva_c, params_c, task="clf")
plt.hist(yva_prob.ravel(), bins=40, alpha=0.6, label="val prob")
plt.hist(yte_prob.ravel(), bins=40, alpha=0.6, label="test prob")
plt.axvline(0.5, linestyle="--")
plt.title("Classification prob. distribution (val/test)")
plt.xlabel("P(valid=1)"); plt.ylabel("count")
plt.legend(); plt.grid(True, alpha=0.25)
plt.show()

# 回歸：真值 vs. 預測（測試集）
plt.figure(figsize=(5,5))
plt.scatter(yte_r.ravel(), yte_hat.ravel(), s=10, alpha=0.5)
mn = min(yte_r.min(), yte_hat.min())
mx = max(yte_r.max(), yte_hat.max())
plt.plot([mn, mx], [mn, mx])
plt.title("Regression: True vs Pred (test)")
plt.xlabel("True"); plt.ylabel("Pred")
plt.grid(True, alpha=0.25)
plt.show()

# 資料分布
plt.figure(figsize=(6,6))
sc = plt.scatter(df_reg["lon"], df_reg["lat"],
                 c=df_reg["value"], cmap="jet", s=10)
plt.colorbar(sc, label="Temperature (°C)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Temperature Distribution (Scatter Plot)")
plt.show()