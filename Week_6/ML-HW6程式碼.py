# -*- coding: utf-8 -*-
"""10/17

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16VsB-eirnbnMuS2gkkUwUQxNbY9aa-B2
"""

# 有使用gpt
import math
import numpy as np
import pandas as pd
from pathlib import Path
from xml.etree import ElementTree as ET
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap

from google.colab import files # 在 Colab 上傳 XML(資料檔)
print("請上傳 O-A0038-003.xml …")
uploaded = files.upload()    # 選檔上傳

# 題目給的參數
LON0 = 120.00    # 左下角經度
LAT0 = 21.88    # 左下角緯度
DLON = 0.03     # 經度解析度
DLAT = 0.03     # 緯度解析度
NX  = 67      # 經度個數
NY  = 120     # 緯度個數
NPTS = NX * NY  # 總資料個數 8040
INVALID_VALUE = -999.0     # 無效值

def _try_parse_number_list(text: str):
    # 目標 : 從檔案抓出長度為8040的一串浮點數
    cand = text.replace(",", " ").split() # 將逗號轉成空白
    try:
        vals = list(map(float, cand))
        if len(vals) == NPTS: # 若資料個數是題目給的8040則回傳
            return vals
    except Exception:
        pass

#讀檔函數
def extract_grid_values_from_xml(xml_path: str):
    root = ET.parse(xml_path).getroot() # 掃描整個檔案且用樹來儲存資料，並給出根節點
    full = ET.tostring(root, encoding="unicode") # 將樹轉成字串
    # 掃描每個節點
    for elem in root.iter():
        for candidate in (elem.text, elem.tail):
            if candidate:
                v = _try_parse_number_list(candidate)
                if v is not None:
                    return np.array(v, dtype=float)
    raise ValueError(f"檔案中找不到剛好 {NPTS} 個數字的資料區塊")


# 建立經緯度網格
def build_lon_lat_grid():
    lons_1d = LON0 + np.arange(NX) * DLON
    lats_1d = LAT0 + np.arange(NY) * DLAT
    lon_grid, lat_grid = np.meshgrid(lons_1d, lats_1d)
    return lon_grid, lat_grid

def make_datasets(vals_1d: np.ndarray):
    arr = vals_1d.reshape(NY, NX)  # (120,67)
    lon_grid, lat_grid = build_lon_lat_grid()

    # 分類
    labels = (arr != INVALID_VALUE).astype(int)
    df_cls = pd.DataFrame({
        "lon": lon_grid.ravel(),
        "lat": lat_grid.ravel(),
        "label": labels.ravel()
    })

    # 回歸（只保留有效的）- 這次的作業不會用到
    mask = (arr != INVALID_VALUE)
    df_reg = pd.DataFrame({
        "lon": lon_grid[mask],
        "lat": lat_grid[mask],
        "value": arr[mask]
    })
    return df_cls, df_reg


# 規範化 ( min-max scaling )
def scale_lonlat_to_m1p1(lon: np.ndarray, lat: np.ndarray):
    lon_min, lon_max = LON0, LON0 + (NX-1)*DLON
    lat_min, lat_max = LAT0, LAT0 + (NY-1)*DLAT
    lon_s = 2.0*(lon - lon_min)/(lon_max - lon_min) - 1.0
    lat_s = 2.0*(lat - lat_min)/(lat_max - lat_min) - 1.0
    return lon_s, lat_s

def fit_qda_binary(X, y, reg=1e-3):
    # reg 是用來防止協方差矩陣Σ奇異
    pi1 = y.mean(); pi0 = 1 - pi1  # y 的先驗機率
    X0, X1 = X[y==0], X[y==1]
    mu0, mu1 = X0.mean(axis=0), X1.mean(axis=0)  # 各類的均值向量 𝜇0,𝜇1
    S0 = np.cov(X0, rowvar=False, bias=True) + reg * np.eye(X.shape[1])  #算協方差矩陣Σ並加上小數值reg避免奇異
    S1 = np.cov(X1, rowvar=False, bias=True) + reg * np.eye(X.shape[1])
    return {"pi0":pi0, "pi1":pi1, "mu0":mu0, "mu1":mu1, "S0":S0, "S1":S1}

def qda_scores(X, P):
    S0i, S1i = np.linalg.pinv(P["S0"]), np.linalg.pinv(P["S1"])  #解Σ的逆矩陣
    _, logdetS0 = np.linalg.slogdet(P["S0"])  #算 log⁡∣det⁡(𝑃["𝑆0"])∣且用穩定的數值法算避免爆掉(AI推薦的)
    _, logdetS1 = np.linalg.slogdet(P["S1"])

    g0 = -0.5 * np.einsum('ij,ij->i', (X-P["mu0"])@S0i, (X-P["mu0"])) - 0.5 * logdetS0 + np.log(P["pi0"])  #算兩類的分數 (分數較大者就是預測的類別)
    g1 = -0.5 * np.einsum('ij,ij->i', (X-P["mu1"])@S1i, (X-P["mu1"])) - 0.5 * logdetS1 + np.log(P["pi1"])
    return g0, g1
def qda_predict(X, P):
    g0, g1 = qda_scores(X, P)
    return (g1 >= g0).astype(int)


def plot_qda_boundary_lonlat(df_cls, P, step=0.01):
    lon = df_cls["lon"].values; lat = df_cls["lat"].values; y = df_cls["label"].values
    lon_min, lon_max = lon.min(), lon.max()
    lat_min, lat_max = lat.min(), lat.max()
    xx, yy = np.meshgrid(np.arange(lon_min, lon_max+1e-9, step),
                         np.arange(lat_min, lat_max+1e-9, step))
    grid = np.c_[xx.ravel(), yy.ravel()]
    g0, g1 = qda_scores(grid, P)
    Z = (g1 >= g0).astype(int).reshape(xx.shape)

    plt.figure(figsize=(6,6))
    plt.pcolormesh(xx, yy, Z, shading='auto', alpha=0.15, cmap=ListedColormap(['#f8c6c6', '#c6d2f8']))
    plt.contour(xx, yy, (g1-g0).reshape(xx.shape), levels=[0], colors=['#fff2cc'], linewidths=3)
    plt.scatter(lon[y==0], lat[y==0], s=8, c='#b2182b', label='No Data', alpha=0.7)
    plt.scatter(lon[y==1], lat[y==1], s=8, c='#2166ac', label='Has Data', alpha=0.7)
    plt.xlabel("Longitude"); plt.ylabel("Latitude")
    plt.title("QDA Decision Boundary and Data Distribution")
    plt.legend(); plt.tight_layout(); plt.show()

# 讀檔
xml_path = "O-A0038-003.xml"
print(f"讀取 XML：{xml_path}")

vals_1d = extract_grid_values_from_xml(xml_path)
print("建立資料集 …")
df_cls , df_reg = make_datasets(vals_1d)

X_ll = df_cls[["lon","lat"]].to_numpy().astype(float)
y_ll = df_cls["label"].to_numpy().astype(int)

rng = np.random.RandomState()
idx = rng.permutation(len(y_ll))
n_tr = int(0.8 * len(y_ll))
tr_idx, te_idx = idx[:n_tr], idx[n_tr:]

X_tr, y_tr = X_ll[tr_idx], y_ll[tr_idx]
X_te, y_te = X_ll[te_idx], y_ll[te_idx]

print(f"train size = {len(y_tr)}, test size = {len(y_te)}")
P = fit_qda_binary(X_tr, y_tr, reg=1e-3)
y_pred_te = qda_predict(X_te, P)
test_acc = (y_pred_te == y_te).mean()

# 混淆矩陣
cm = np.zeros((2,2), dtype=int)
for yt, yp in zip(y_te, y_pred_te):
    cm[yt, yp] += 1

print(f"QDA test accuracy = {test_acc:.4f}")
print("Confusion matrix on test [rows=true, cols=pred]:")
print(cm)

plot_qda_boundary_lonlat(df_cls, P, step=0.01)#畫決策邊界

# 使用稍微修改過的上次作業的神經網路(四層，兩個隱藏層神經元皆為10個)
def sigmoid(x):
    return 1.0/(1.0 + np.exp(-x))

def Weight_Initialization(m, n, generator):
    limit = 5.0
    W = generator.uniform(-limit, limit, size=(m, n))
    b = generator.uniform(-limit, limit, size=(m, 1))
    return W, b

def forward_pass(x, params, task="reg"):
    W2, b2 = params["W2"], params["b2"]
    W3, b3 = params["W3"], params["b3"]
    W4, b4 = params["W4"], params["b4"]
    z2 = W2 @ x + b2; a2 = sigmoid(z2)
    z3 = W3 @ a2 + b3; a3 = sigmoid(z3)
    z4 = W4 @ a3 + b4
    y_hat = sigmoid(z4) if task=="clf" else z4 # 分類模型的輸出會再加上一個sigmoid函數(使其輸出在0到1之間)
    return y_hat, (a2, a3)

def train_twohidden_mlp(
    X_tr, y_tr,
    X_val=None, y_val=None,
    task="reg", in_dim=2, H1=10, H2=10, out_dim=1,
    epochs=100, batch_size=16, lr=0.05,
    pos_weight=None
):
    rng = np.random.default_rng()
    W2, b2 = Weight_Initialization(H1, in_dim, rng)
    W3, b3 = Weight_Initialization(H2, H1, rng)
    W4, b4 = Weight_Initialization(out_dim, H2, rng)
    params = {"W2": W2, "b2": b2, "W3": W3, "b3": b3, "W4": W4, "b4": b4}

    def mse_loss(yh, y): return np.mean((yh - y)**2)
    def bce_loss(yh, y, eps=1e-9, pw = pos_weight): # 參考 CSDN 的 BCE Loss
        yh = np.clip(yh, eps, 1.0-eps) # 避免 log(0) 出現
        return -np.mean( pw * y * np.log(yh) + (1-y) * np.log(1-yh))
    if task == "clf":
        if pos_weight is None:
            y_flat = y_tr.ravel()
            pos = float((y_flat == 1).sum())
            neg = float((y_flat == 0).sum())
            pos_weight = (neg / pos) if pos > 0 else 1.0
        # 保險（防止遇到如 pos=0 的極端情形）
        if not np.isfinite(pos_weight):
            pos_weight = 1.0

    hist_tr, hist_va = [], []
    N = X_tr.shape[1]

    for ep in range(1, epochs+1):
        idx = rng.permutation(N)
        total = 0.0
        for i in range(0, N, batch_size):
            ib = idx[i:i+batch_size]
            xb = X_tr[:, ib]      # (in_dim, B)
            yb = y_tr[:, ib]      # (out_dim, B)
            B  = xb.shape[1]

            yh, (a2,a3) = forward_pass(xb, params, task=task)
            if task=="reg":
                dL_dy = 2.0*(yh - yb)/B
                dL_dz4 = dL_dy
                total += mse_loss(yh, yb)*B
            else:
                eps = 1e-9
                yh_c = np.clip(yh, eps, 1.0-eps)
                total += bce_loss(yh_c, yb, pw=pos_weight)*B
                dL_dyh = -(pos_weight * yb / yh_c) + ((1.0 - yb) / (1.0 - yh_c))
                dL_dz4 = dL_dyh * (yh_c * (1.0 - yh_c))

            dL_dW4 = dL_dz4 @ a3.T
            dL_db4 = np.sum(dL_dz4, axis=1, keepdims=True)

            dL_da3 = params["W4"].T @ dL_dz4
            dL_dz3 = dL_da3 * (a3 * (1.0 - a3))
            dL_dW3 = dL_dz3 @ a2.T
            dL_db3 = np.sum(dL_dz3, axis=1, keepdims=True)

            dL_da2 = params["W3"].T @ dL_dz3
            dL_dz2 = dL_da2 * (a2 * (1.0 - a2))
            dL_dW2 = dL_dz2 @ xb.T
            dL_db2 = np.sum(dL_dz2, axis=1, keepdims=True)

            params["W4"] -= lr * dL_dW4; params["b4"] -= lr * dL_db4
            params["W3"] -= lr * dL_dW3; params["b3"] -= lr * dL_db3
            params["W2"] -= lr * dL_dW2; params["b2"] -= lr * dL_db2

        tr_loss = total/N
        hist_tr.append(tr_loss)
        if X_val is not None and y_val is not None:
            yh_v, _ = forward_pass(X_val, params, task=task)
            va_loss = mse_loss(yh_v, y_val) if task=="reg" else bce_loss(yh_v, y_val, pw=pos_weight)
        else:
            va_loss = np.nan
        hist_va.append(va_loss)

        if ep % 10 == 0: # 每十個印一次結果
            if task=="reg":
                rmse = np.sqrt(np.mean((yh_v - y_val)**2)) if X_val is not None else float('nan')
                print(f"[回歸 ep {ep:03d}] train_mse={tr_loss:.4f}  val_mse={va_loss:.4f}  val_rmse={rmse:.3f}")
            else:
                acc = None
                if X_val is not None:
                    acc = ((yh_v>=0.5).astype(int)==y_val).mean()
                print(f"[分類 ep {ep:03d}] train_bce={tr_loss:.4f}  val_bce={va_loss:.4f}  val_acc={acc if acc is not None else float('nan'):.4f}")
    return params, {"train_loss": hist_tr, "val_loss": hist_va}


# 讀檔
#xml_path = "O-A0038-003.xml"
#print(f"讀取 XML：{xml_path}")

#vals_1d = extract_grid_values_from_xml(xml_path)
#print("建立資料集 …")
#df_cls, df_reg = make_datasets(vals_1d)

# 輸出 CSV
df_cls.to_csv("classification_dataset.csv", index=False, encoding="utf-8")
df_reg.to_csv("regression_dataset.csv",   index=False, encoding="utf-8")
print("已輸出：classification_dataset.csv")
print("已輸出：regression_dataset.csv")
print(f"分類資料集大小：{df_cls.shape}（含無效/有效點）")
print(f"回歸資料集大小：{df_reg.shape}（僅有效點）")


# 準備分類模型要用到的資料並縮放到 [-1,1]
lon_s, lat_s = scale_lonlat_to_m1p1(df_cls["lon"].values, df_cls["lat"].values)
Xc = np.vstack([lon_s, lat_s])         # (2, N)
yc = df_cls["label"].values.reshape(1, -1)   # (1, N)

# 打亂資料順序
rng = np.random.default_rng()
N = Xc.shape[1]
perm = rng.permutation(N)

# 訓練集資料量 : 驗證集 : 測試集 = 6 : 2 : 2
n_tr = int(0.6 * N); n_va = int(0.2 * N)
id_tr, id_va, id_te = perm[:n_tr], perm[n_tr:n_tr+n_va], perm[n_tr+n_va:]
Xtr_c, Xva_c, Xte_c = Xc[:, id_tr], Xc[:, id_va], Xc[:, id_te]
ytr_c, yva_c, yte_c = yc[:, id_tr], yc[:, id_va], yc[:, id_te]

# 訓練分類模型
params_c, hist_c = train_twohidden_mlp(
    X_tr=Xtr_c, y_tr=ytr_c, X_val=Xva_c, y_val=yva_c,
    task="clf", in_dim=2, H1=10, H2=10, out_dim=1,
    epochs=300, batch_size=16, lr=0.01
)
yte_prob, _ = forward_pass(Xte_c, params_c, task="clf")
yte_pred = (yte_prob >= 0.5).astype(int)
acc_te = (yte_pred == yte_c).mean()
tn = int(((yte_c==0)&(yte_pred==0)).sum())
fp = int(((yte_c==0)&(yte_pred==1)).sum())
fn = int(((yte_c==1)&(yte_pred==0)).sum())
tp = int(((yte_c==1)&(yte_pred==1)).sum())
cm = np.array([[tn, fp],[fn, tp]])
print("\n=== 分類模型 ===")
print(f"Test Accuracy: {acc_te:.4f}")
print("Confusion Matrix:\n", cm)

# 準備回歸模型要用到的資料並縮放到 [-1,1]
lon_sr, lat_sr = scale_lonlat_to_m1p1(df_reg["lon"].values, df_reg["lat"].values)
Xr = np.vstack([lon_sr, lat_sr])          # (2, Nr)
yr = df_reg["value"].values.reshape(1, -1)     # (1, Nr)

Nr = Xr.shape[1]
perm_r = rng.permutation(Nr)
ntr_r = int(0.6*Nr); nva_r = int(0.2*Nr)
id_tr_r, id_va_r, id_te_r = perm_r[:ntr_r], perm_r[ntr_r:ntr_r+nva_r], perm_r[ntr_r+nva_r:]
Xtr_r, Xva_r, Xte_r = Xr[:, id_tr_r], Xr[:, id_va_r], Xr[:, id_te_r]
ytr_r, yva_r, yte_r = yr[:, id_tr_r], yr[:, id_va_r], yr[:, id_te_r]

# 訓練回歸模型
params_r, hist_r = train_twohidden_mlp(
    X_tr=Xtr_r, y_tr=ytr_r, X_val=Xva_r, y_val=yva_r,
    task="reg", in_dim=2, H1=10, H2=10, out_dim=1,
    epochs=800, batch_size=4, lr=0.005
)
yte_hat, _ = forward_pass(Xte_r, params_r, task="reg")
rmse = float(np.sqrt(np.mean((yte_hat - yte_r)**2)))
r2  = float(1.0 - np.sum((yte_hat - yte_r)**2)/np.sum((yte_r - yte_r.mean())**2))
print("\n=== 回歸模型 ===")
print(f"Test RMSE: {rmse:.3f}")
print(f"Test R^2 : {r2:.3f}")

# 分類損失曲線
plt.figure(figsize=(7,5))
plt.plot(range(1, len(hist_c["train_loss"])+1), hist_c["train_loss"], label="Train Loss")
plt.plot(range(1, len(hist_c["val_loss"])+1),   hist_c["val_loss"],   label="Val Loss")
plt.title("Classification - Loss Curve")
plt.xlabel("Epoch"); plt.ylabel("Loss (BCE)")
plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()
plt.show()

# 回歸損失曲線
plt.figure(figsize=(7,5))
plt.plot(range(1, len(hist_r["train_loss"])+1), hist_r["train_loss"], label="Train Loss")
plt.plot(range(1, len(hist_r["val_loss"])+1),   hist_r["val_loss"],   label="Val Loss")
plt.title("Regression - Loss Curve")
plt.xlabel("Epoch"); plt.ylabel("Loss (MSE)")
plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()
plt.show()

# 分類：驗證/測試機率分佈
#plt.figure(figsize=(6,4))
#yva_prob, _ = forward_pass(Xva_c, params_c, task="clf")
#plt.hist(yva_prob.ravel(), bins=40, alpha=0.6, label="val prob")
#plt.hist(yte_prob.ravel(), bins=40, alpha=0.6, label="test prob")
#plt.axvline(0.5, linestyle="--")
#plt.title("Classification prob. distribution (val/test)")
#plt.xlabel("P(valid=1)"); plt.ylabel("count")
#plt.legend(); plt.grid(True, alpha=0.25)
#plt.show()

# 回歸：真值 vs. 預測（測試集）
#plt.figure(figsize=(5,5))
#plt.scatter(yte_r.ravel(), yte_hat.ravel(), s=10, alpha=0.5)
#mn = min(yte_r.min(), yte_hat.min())
#mx = max(yte_r.max(), yte_hat.max())
#plt.plot([mn, mx], [mn, mx])
#plt.title("Regression: True vs Pred (test)")
#plt.xlabel("True"); plt.ylabel("Pred")
#plt.grid(True, alpha=0.25)
#plt.show()

INVALID_VALUE = -999.0

lon_all = df_cls["lon"].values
lat_all = df_cls["lat"].values
lon_s_all , lat_s_all = scale_lonlat_to_m1p1(lon_all, lat_all)
X_all = np.vstack([lon_s_all, lat_s_all])

# h(x)
p_all, _ = forward_pass(X_all, params_c, task="clf")
p_all = p_all.ravel()
cls_all = (p_all >= 0.5)
h = np.full(p_all.shape[0], INVALID_VALUE, dtype=float)
if cls_all.any():
    y_reg_all , _ = forward_pass(X_all[:, cls_all], params_r, task="reg")
    h[cls_all] = y_reg_all.ravel()

lon_u = np.sort(df_cls["lon"].unique())
lat_u = np.sort(df_cls["lat"].unique())
H = h.reshape(len(lat_u), len(lon_u))

H_masked = np.ma.masked_where(H == INVALID_VALUE, H)
cmap = mpl.cm.get_cmap("jet").copy()
cmap.set_bad(color="black") # 令 -999 顯示黑色

plt.figure(figsize=(6,6))
m = plt.pcolormesh(lon_u, lat_u, H_masked, shading="auto", cmap=cmap)
plt.xlabel("Longitude"); plt.ylabel("Latitude")
plt.title("Piecewise Temperature Map: h(x) (jet colormap)")
cb = plt.colorbar(m, label="Temperature (°C)")

# -999 的圖例
invalid_patch = mpatches.Patch(color="black" , label="Invalid (-999)")
plt.legend(handles=[invalid_patch] , loc="upper right")

df_true = (
    df_cls[["lon","lat"]]
    .merge(df_reg[["lon","lat","value"]], on=["lon","lat"], how="left")
)

# 真實溫度(沒有值的話設為-999)
y_true_all = df_true["value"].fillna(INVALID_VALUE).to_numpy(dtype=float)  # shape (N,)

# h(x)的預測溫度與真實溫度比較(全部的點)，計算準確率(誤差在±1度內算正確)
diff = h - y_true_all
near_mask_all = (np.abs(diff) <= 1.0)
num_correct_all = int(near_mask_all.sum())
total_pts = int(near_mask_all.size)
acc_1deg_all = num_correct_all / total_pts
print(f"[±1°C accuracy | all points] {num_correct_all} / {total_pts} ({acc_1deg_all:.2%})")

# 只在有數值的點計算準確率(誤差在±1度內算正確)
true_valid = (y_true_all != INVALID_VALUE)
near_mask_valid = (np.abs(diff[true_valid]) <= 1.0)
num_correct_valid = int(near_mask_valid.sum())
total_valid = int(true_valid.sum())
acc_1deg_valid = num_correct_valid / total_valid
print(f"[±1°C accuracy | valid-only] {num_correct_valid} / {total_valid} ({acc_1deg_valid:.2%})")

# 放圖左下角
ax = plt.gca()
ax.text(
    0.01, 0.01,
    f"±1°C Accuracy (all): {num_correct_all}/{total_pts} ({acc_1deg_all:.1%})"
    + (f"\n±1°C Accuracy (valid): {num_correct_valid}/{total_valid} ({acc_1deg_valid:.1%})" if np.isfinite(acc_1deg_valid) else ""),
    transform=ax.transAxes, ha="left", va="bottom",
    bbox=dict(boxstyle="round", fc="white", alpha=0.85, ec="none"),
    fontsize=10)

plt.tight_layout()
plt.show()

# 資料分布
plt.figure(figsize=(6,6))
sc = plt.scatter(df_reg["lon"], df_reg["lat"],
                 c=df_reg["value"], cmap="jet", s=10)
plt.colorbar(sc, label="Temperature (°C)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Temperature Distribution (Scatter Plot)")
plt.show()