# -*- coding: utf-8 -*-
"""10/17

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16VsB-eirnbnMuS2gkkUwUQxNbY9aa-B2
"""

# æœ‰ä½¿ç”¨gpt
import math
import numpy as np
import pandas as pd
from pathlib import Path
from xml.etree import ElementTree as ET
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap

from google.colab import files # åœ¨ Colab ä¸Šå‚³ XML(è³‡æ–™æª”)
print("è«‹ä¸Šå‚³ O-A0038-003.xml â€¦")
uploaded = files.upload()    # é¸æª”ä¸Šå‚³

# é¡Œç›®çµ¦çš„åƒæ•¸
LON0 = 120.00    # å·¦ä¸‹è§’ç¶“åº¦
LAT0 = 21.88    # å·¦ä¸‹è§’ç·¯åº¦
DLON = 0.03     # ç¶“åº¦è§£æåº¦
DLAT = 0.03     # ç·¯åº¦è§£æåº¦
NX  = 67      # ç¶“åº¦å€‹æ•¸
NY  = 120     # ç·¯åº¦å€‹æ•¸
NPTS = NX * NY  # ç¸½è³‡æ–™å€‹æ•¸ 8040
INVALID_VALUE = -999.0     # ç„¡æ•ˆå€¼

def _try_parse_number_list(text: str):
    # ç›®æ¨™ : å¾æª”æ¡ˆæŠ“å‡ºé•·åº¦ç‚º8040çš„ä¸€ä¸²æµ®é»æ•¸
    cand = text.replace(",", " ").split() # å°‡é€—è™Ÿè½‰æˆç©ºç™½
    try:
        vals = list(map(float, cand))
        if len(vals) == NPTS: # è‹¥è³‡æ–™å€‹æ•¸æ˜¯é¡Œç›®çµ¦çš„8040å‰‡å›å‚³
            return vals
    except Exception:
        pass

#è®€æª”å‡½æ•¸
def extract_grid_values_from_xml(xml_path: str):
    root = ET.parse(xml_path).getroot() # æƒææ•´å€‹æª”æ¡ˆä¸”ç”¨æ¨¹ä¾†å„²å­˜è³‡æ–™ï¼Œä¸¦çµ¦å‡ºæ ¹ç¯€é»
    full = ET.tostring(root, encoding="unicode") # å°‡æ¨¹è½‰æˆå­—ä¸²
    # æƒææ¯å€‹ç¯€é»
    for elem in root.iter():
        for candidate in (elem.text, elem.tail):
            if candidate:
                v = _try_parse_number_list(candidate)
                if v is not None:
                    return np.array(v, dtype=float)
    raise ValueError(f"æª”æ¡ˆä¸­æ‰¾ä¸åˆ°å‰›å¥½ {NPTS} å€‹æ•¸å­—çš„è³‡æ–™å€å¡Š")


# å»ºç«‹ç¶“ç·¯åº¦ç¶²æ ¼
def build_lon_lat_grid():
    lons_1d = LON0 + np.arange(NX) * DLON
    lats_1d = LAT0 + np.arange(NY) * DLAT
    lon_grid, lat_grid = np.meshgrid(lons_1d, lats_1d)
    return lon_grid, lat_grid

def make_datasets(vals_1d: np.ndarray):
    arr = vals_1d.reshape(NY, NX)  # (120,67)
    lon_grid, lat_grid = build_lon_lat_grid()

    # åˆ†é¡
    labels = (arr != INVALID_VALUE).astype(int)
    df_cls = pd.DataFrame({
        "lon": lon_grid.ravel(),
        "lat": lat_grid.ravel(),
        "label": labels.ravel()
    })

    # å›æ­¸ï¼ˆåªä¿ç•™æœ‰æ•ˆçš„ï¼‰- é€™æ¬¡çš„ä½œæ¥­ä¸æœƒç”¨åˆ°
    mask = (arr != INVALID_VALUE)
    df_reg = pd.DataFrame({
        "lon": lon_grid[mask],
        "lat": lat_grid[mask],
        "value": arr[mask]
    })
    return df_cls, df_reg


# è¦ç¯„åŒ– ( min-max scaling )
def scale_lonlat_to_m1p1(lon: np.ndarray, lat: np.ndarray):
    lon_min, lon_max = LON0, LON0 + (NX-1)*DLON
    lat_min, lat_max = LAT0, LAT0 + (NY-1)*DLAT
    lon_s = 2.0*(lon - lon_min)/(lon_max - lon_min) - 1.0
    lat_s = 2.0*(lat - lat_min)/(lat_max - lat_min) - 1.0
    return lon_s, lat_s

def fit_qda_binary(X, y, reg=1e-3):
    # reg æ˜¯ç”¨ä¾†é˜²æ­¢å”æ–¹å·®çŸ©é™£Î£å¥‡ç•°
    pi1 = y.mean(); pi0 = 1 - pi1  # y çš„å…ˆé©—æ©Ÿç‡
    X0, X1 = X[y==0], X[y==1]
    mu0, mu1 = X0.mean(axis=0), X1.mean(axis=0)  # å„é¡çš„å‡å€¼å‘é‡ ğœ‡0,ğœ‡1
    S0 = np.cov(X0, rowvar=False, bias=True) + reg * np.eye(X.shape[1])  #ç®—å”æ–¹å·®çŸ©é™£Î£ä¸¦åŠ ä¸Šå°æ•¸å€¼regé¿å…å¥‡ç•°
    S1 = np.cov(X1, rowvar=False, bias=True) + reg * np.eye(X.shape[1])
    return {"pi0":pi0, "pi1":pi1, "mu0":mu0, "mu1":mu1, "S0":S0, "S1":S1}

def qda_scores(X, P):
    S0i, S1i = np.linalg.pinv(P["S0"]), np.linalg.pinv(P["S1"])  #è§£Î£çš„é€†çŸ©é™£
    _, logdetS0 = np.linalg.slogdet(P["S0"])  #ç®— logâ¡âˆ£detâ¡(ğ‘ƒ["ğ‘†0"])âˆ£ä¸”ç”¨ç©©å®šçš„æ•¸å€¼æ³•ç®—é¿å…çˆ†æ‰(AIæ¨è–¦çš„)
    _, logdetS1 = np.linalg.slogdet(P["S1"])

    g0 = -0.5 * np.einsum('ij,ij->i', (X-P["mu0"])@S0i, (X-P["mu0"])) - 0.5 * logdetS0 + np.log(P["pi0"])  #ç®—å…©é¡çš„åˆ†æ•¸ (åˆ†æ•¸è¼ƒå¤§è€…å°±æ˜¯é æ¸¬çš„é¡åˆ¥)
    g1 = -0.5 * np.einsum('ij,ij->i', (X-P["mu1"])@S1i, (X-P["mu1"])) - 0.5 * logdetS1 + np.log(P["pi1"])
    return g0, g1
def qda_predict(X, P):
    g0, g1 = qda_scores(X, P)
    return (g1 >= g0).astype(int)


def plot_qda_boundary_lonlat(df_cls, P, step=0.01):
    lon = df_cls["lon"].values; lat = df_cls["lat"].values; y = df_cls["label"].values
    lon_min, lon_max = lon.min(), lon.max()
    lat_min, lat_max = lat.min(), lat.max()
    xx, yy = np.meshgrid(np.arange(lon_min, lon_max+1e-9, step),
                         np.arange(lat_min, lat_max+1e-9, step))
    grid = np.c_[xx.ravel(), yy.ravel()]
    g0, g1 = qda_scores(grid, P)
    Z = (g1 >= g0).astype(int).reshape(xx.shape)

    plt.figure(figsize=(6,6))
    plt.pcolormesh(xx, yy, Z, shading='auto', alpha=0.15, cmap=ListedColormap(['#f8c6c6', '#c6d2f8']))
    plt.contour(xx, yy, (g1-g0).reshape(xx.shape), levels=[0], colors=['#fff2cc'], linewidths=3)
    plt.scatter(lon[y==0], lat[y==0], s=8, c='#b2182b', label='No Data', alpha=0.7)
    plt.scatter(lon[y==1], lat[y==1], s=8, c='#2166ac', label='Has Data', alpha=0.7)
    plt.xlabel("Longitude"); plt.ylabel("Latitude")
    plt.title("QDA Decision Boundary and Data Distribution")
    plt.legend(); plt.tight_layout(); plt.show()

# è®€æª”
xml_path = "O-A0038-003.xml"
print(f"è®€å– XMLï¼š{xml_path}")

vals_1d = extract_grid_values_from_xml(xml_path)
print("å»ºç«‹è³‡æ–™é›† â€¦")
df_cls , df_reg = make_datasets(vals_1d)

X_ll = df_cls[["lon","lat"]].to_numpy().astype(float)
y_ll = df_cls["label"].to_numpy().astype(int)

rng = np.random.RandomState()
idx = rng.permutation(len(y_ll))
n_tr = int(0.8 * len(y_ll))
tr_idx, te_idx = idx[:n_tr], idx[n_tr:]

X_tr, y_tr = X_ll[tr_idx], y_ll[tr_idx]
X_te, y_te = X_ll[te_idx], y_ll[te_idx]

print(f"train size = {len(y_tr)}, test size = {len(y_te)}")
P = fit_qda_binary(X_tr, y_tr, reg=1e-3)
y_pred_te = qda_predict(X_te, P)
test_acc = (y_pred_te == y_te).mean()

# æ··æ·†çŸ©é™£
cm = np.zeros((2,2), dtype=int)
for yt, yp in zip(y_te, y_pred_te):
    cm[yt, yp] += 1

print(f"QDA test accuracy = {test_acc:.4f}")
print("Confusion matrix on test [rows=true, cols=pred]:")
print(cm)

plot_qda_boundary_lonlat(df_cls, P, step=0.01)#ç•«æ±ºç­–é‚Šç•Œ

# ä½¿ç”¨ç¨å¾®ä¿®æ”¹éçš„ä¸Šæ¬¡ä½œæ¥­çš„ç¥ç¶“ç¶²è·¯(å››å±¤ï¼Œå…©å€‹éš±è—å±¤ç¥ç¶“å…ƒçš†ç‚º10å€‹)
def sigmoid(x):
    return 1.0/(1.0 + np.exp(-x))

def Weight_Initialization(m, n, generator):
    limit = 5.0
    W = generator.uniform(-limit, limit, size=(m, n))
    b = generator.uniform(-limit, limit, size=(m, 1))
    return W, b

def forward_pass(x, params, task="reg"):
    W2, b2 = params["W2"], params["b2"]
    W3, b3 = params["W3"], params["b3"]
    W4, b4 = params["W4"], params["b4"]
    z2 = W2 @ x + b2; a2 = sigmoid(z2)
    z3 = W3 @ a2 + b3; a3 = sigmoid(z3)
    z4 = W4 @ a3 + b4
    y_hat = sigmoid(z4) if task=="clf" else z4 # åˆ†é¡æ¨¡å‹çš„è¼¸å‡ºæœƒå†åŠ ä¸Šä¸€å€‹sigmoidå‡½æ•¸(ä½¿å…¶è¼¸å‡ºåœ¨0åˆ°1ä¹‹é–“)
    return y_hat, (a2, a3)

def train_twohidden_mlp(
    X_tr, y_tr,
    X_val=None, y_val=None,
    task="reg", in_dim=2, H1=10, H2=10, out_dim=1,
    epochs=100, batch_size=16, lr=0.05,
    pos_weight=None
):
    rng = np.random.default_rng()
    W2, b2 = Weight_Initialization(H1, in_dim, rng)
    W3, b3 = Weight_Initialization(H2, H1, rng)
    W4, b4 = Weight_Initialization(out_dim, H2, rng)
    params = {"W2": W2, "b2": b2, "W3": W3, "b3": b3, "W4": W4, "b4": b4}

    def mse_loss(yh, y): return np.mean((yh - y)**2)
    def bce_loss(yh, y, eps=1e-9, pw = pos_weight): # åƒè€ƒ CSDN çš„ BCE Loss
        yh = np.clip(yh, eps, 1.0-eps) # é¿å… log(0) å‡ºç¾
        return -np.mean( pw * y * np.log(yh) + (1-y) * np.log(1-yh))
    if task == "clf":
        if pos_weight is None:
            y_flat = y_tr.ravel()
            pos = float((y_flat == 1).sum())
            neg = float((y_flat == 0).sum())
            pos_weight = (neg / pos) if pos > 0 else 1.0
        # ä¿éšªï¼ˆé˜²æ­¢é‡åˆ°å¦‚ pos=0 çš„æ¥µç«¯æƒ…å½¢ï¼‰
        if not np.isfinite(pos_weight):
            pos_weight = 1.0

    hist_tr, hist_va = [], []
    N = X_tr.shape[1]

    for ep in range(1, epochs+1):
        idx = rng.permutation(N)
        total = 0.0
        for i in range(0, N, batch_size):
            ib = idx[i:i+batch_size]
            xb = X_tr[:, ib]      # (in_dim, B)
            yb = y_tr[:, ib]      # (out_dim, B)
            B  = xb.shape[1]

            yh, (a2,a3) = forward_pass(xb, params, task=task)
            if task=="reg":
                dL_dy = 2.0*(yh - yb)/B
                dL_dz4 = dL_dy
                total += mse_loss(yh, yb)*B
            else:
                eps = 1e-9
                yh_c = np.clip(yh, eps, 1.0-eps)
                total += bce_loss(yh_c, yb, pw=pos_weight)*B
                dL_dyh = -(pos_weight * yb / yh_c) + ((1.0 - yb) / (1.0 - yh_c))
                dL_dz4 = dL_dyh * (yh_c * (1.0 - yh_c))

            dL_dW4 = dL_dz4 @ a3.T
            dL_db4 = np.sum(dL_dz4, axis=1, keepdims=True)

            dL_da3 = params["W4"].T @ dL_dz4
            dL_dz3 = dL_da3 * (a3 * (1.0 - a3))
            dL_dW3 = dL_dz3 @ a2.T
            dL_db3 = np.sum(dL_dz3, axis=1, keepdims=True)

            dL_da2 = params["W3"].T @ dL_dz3
            dL_dz2 = dL_da2 * (a2 * (1.0 - a2))
            dL_dW2 = dL_dz2 @ xb.T
            dL_db2 = np.sum(dL_dz2, axis=1, keepdims=True)

            params["W4"] -= lr * dL_dW4; params["b4"] -= lr * dL_db4
            params["W3"] -= lr * dL_dW3; params["b3"] -= lr * dL_db3
            params["W2"] -= lr * dL_dW2; params["b2"] -= lr * dL_db2

        tr_loss = total/N
        hist_tr.append(tr_loss)
        if X_val is not None and y_val is not None:
            yh_v, _ = forward_pass(X_val, params, task=task)
            va_loss = mse_loss(yh_v, y_val) if task=="reg" else bce_loss(yh_v, y_val, pw=pos_weight)
        else:
            va_loss = np.nan
        hist_va.append(va_loss)

        if ep % 10 == 0: # æ¯åå€‹å°ä¸€æ¬¡çµæœ
            if task=="reg":
                rmse = np.sqrt(np.mean((yh_v - y_val)**2)) if X_val is not None else float('nan')
                print(f"[å›æ­¸ ep {ep:03d}] train_mse={tr_loss:.4f}  val_mse={va_loss:.4f}  val_rmse={rmse:.3f}")
            else:
                acc = None
                if X_val is not None:
                    acc = ((yh_v>=0.5).astype(int)==y_val).mean()
                print(f"[åˆ†é¡ ep {ep:03d}] train_bce={tr_loss:.4f}  val_bce={va_loss:.4f}  val_acc={acc if acc is not None else float('nan'):.4f}")
    return params, {"train_loss": hist_tr, "val_loss": hist_va}


# è®€æª”
#xml_path = "O-A0038-003.xml"
#print(f"è®€å– XMLï¼š{xml_path}")

#vals_1d = extract_grid_values_from_xml(xml_path)
#print("å»ºç«‹è³‡æ–™é›† â€¦")
#df_cls, df_reg = make_datasets(vals_1d)

# è¼¸å‡º CSV
df_cls.to_csv("classification_dataset.csv", index=False, encoding="utf-8")
df_reg.to_csv("regression_dataset.csv",   index=False, encoding="utf-8")
print("å·²è¼¸å‡ºï¼šclassification_dataset.csv")
print("å·²è¼¸å‡ºï¼šregression_dataset.csv")
print(f"åˆ†é¡è³‡æ–™é›†å¤§å°ï¼š{df_cls.shape}ï¼ˆå«ç„¡æ•ˆ/æœ‰æ•ˆé»ï¼‰")
print(f"å›æ­¸è³‡æ–™é›†å¤§å°ï¼š{df_reg.shape}ï¼ˆåƒ…æœ‰æ•ˆé»ï¼‰")


# æº–å‚™åˆ†é¡æ¨¡å‹è¦ç”¨åˆ°çš„è³‡æ–™ä¸¦ç¸®æ”¾åˆ° [-1,1]
lon_s, lat_s = scale_lonlat_to_m1p1(df_cls["lon"].values, df_cls["lat"].values)
Xc = np.vstack([lon_s, lat_s])         # (2, N)
yc = df_cls["label"].values.reshape(1, -1)   # (1, N)

# æ‰“äº‚è³‡æ–™é †åº
rng = np.random.default_rng()
N = Xc.shape[1]
perm = rng.permutation(N)

# è¨“ç·´é›†è³‡æ–™é‡ : é©—è­‰é›† : æ¸¬è©¦é›† = 6 : 2 : 2
n_tr = int(0.6 * N); n_va = int(0.2 * N)
id_tr, id_va, id_te = perm[:n_tr], perm[n_tr:n_tr+n_va], perm[n_tr+n_va:]
Xtr_c, Xva_c, Xte_c = Xc[:, id_tr], Xc[:, id_va], Xc[:, id_te]
ytr_c, yva_c, yte_c = yc[:, id_tr], yc[:, id_va], yc[:, id_te]

# è¨“ç·´åˆ†é¡æ¨¡å‹
params_c, hist_c = train_twohidden_mlp(
    X_tr=Xtr_c, y_tr=ytr_c, X_val=Xva_c, y_val=yva_c,
    task="clf", in_dim=2, H1=10, H2=10, out_dim=1,
    epochs=300, batch_size=16, lr=0.01
)
yte_prob, _ = forward_pass(Xte_c, params_c, task="clf")
yte_pred = (yte_prob >= 0.5).astype(int)
acc_te = (yte_pred == yte_c).mean()
tn = int(((yte_c==0)&(yte_pred==0)).sum())
fp = int(((yte_c==0)&(yte_pred==1)).sum())
fn = int(((yte_c==1)&(yte_pred==0)).sum())
tp = int(((yte_c==1)&(yte_pred==1)).sum())
cm = np.array([[tn, fp],[fn, tp]])
print("\n=== åˆ†é¡æ¨¡å‹ ===")
print(f"Test Accuracy: {acc_te:.4f}")
print("Confusion Matrix:\n", cm)

# æº–å‚™å›æ­¸æ¨¡å‹è¦ç”¨åˆ°çš„è³‡æ–™ä¸¦ç¸®æ”¾åˆ° [-1,1]
lon_sr, lat_sr = scale_lonlat_to_m1p1(df_reg["lon"].values, df_reg["lat"].values)
Xr = np.vstack([lon_sr, lat_sr])          # (2, Nr)
yr = df_reg["value"].values.reshape(1, -1)     # (1, Nr)

Nr = Xr.shape[1]
perm_r = rng.permutation(Nr)
ntr_r = int(0.6*Nr); nva_r = int(0.2*Nr)
id_tr_r, id_va_r, id_te_r = perm_r[:ntr_r], perm_r[ntr_r:ntr_r+nva_r], perm_r[ntr_r+nva_r:]
Xtr_r, Xva_r, Xte_r = Xr[:, id_tr_r], Xr[:, id_va_r], Xr[:, id_te_r]
ytr_r, yva_r, yte_r = yr[:, id_tr_r], yr[:, id_va_r], yr[:, id_te_r]

# è¨“ç·´å›æ­¸æ¨¡å‹
params_r, hist_r = train_twohidden_mlp(
    X_tr=Xtr_r, y_tr=ytr_r, X_val=Xva_r, y_val=yva_r,
    task="reg", in_dim=2, H1=10, H2=10, out_dim=1,
    epochs=800, batch_size=4, lr=0.005
)
yte_hat, _ = forward_pass(Xte_r, params_r, task="reg")
rmse = float(np.sqrt(np.mean((yte_hat - yte_r)**2)))
r2  = float(1.0 - np.sum((yte_hat - yte_r)**2)/np.sum((yte_r - yte_r.mean())**2))
print("\n=== å›æ­¸æ¨¡å‹ ===")
print(f"Test RMSE: {rmse:.3f}")
print(f"Test R^2 : {r2:.3f}")

# åˆ†é¡æå¤±æ›²ç·š
plt.figure(figsize=(7,5))
plt.plot(range(1, len(hist_c["train_loss"])+1), hist_c["train_loss"], label="Train Loss")
plt.plot(range(1, len(hist_c["val_loss"])+1),   hist_c["val_loss"],   label="Val Loss")
plt.title("Classification - Loss Curve")
plt.xlabel("Epoch"); plt.ylabel("Loss (BCE)")
plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()
plt.show()

# å›æ­¸æå¤±æ›²ç·š
plt.figure(figsize=(7,5))
plt.plot(range(1, len(hist_r["train_loss"])+1), hist_r["train_loss"], label="Train Loss")
plt.plot(range(1, len(hist_r["val_loss"])+1),   hist_r["val_loss"],   label="Val Loss")
plt.title("Regression - Loss Curve")
plt.xlabel("Epoch"); plt.ylabel("Loss (MSE)")
plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()
plt.show()

# åˆ†é¡ï¼šé©—è­‰/æ¸¬è©¦æ©Ÿç‡åˆ†ä½ˆ
#plt.figure(figsize=(6,4))
#yva_prob, _ = forward_pass(Xva_c, params_c, task="clf")
#plt.hist(yva_prob.ravel(), bins=40, alpha=0.6, label="val prob")
#plt.hist(yte_prob.ravel(), bins=40, alpha=0.6, label="test prob")
#plt.axvline(0.5, linestyle="--")
#plt.title("Classification prob. distribution (val/test)")
#plt.xlabel("P(valid=1)"); plt.ylabel("count")
#plt.legend(); plt.grid(True, alpha=0.25)
#plt.show()

# å›æ­¸ï¼šçœŸå€¼ vs. é æ¸¬ï¼ˆæ¸¬è©¦é›†ï¼‰
#plt.figure(figsize=(5,5))
#plt.scatter(yte_r.ravel(), yte_hat.ravel(), s=10, alpha=0.5)
#mn = min(yte_r.min(), yte_hat.min())
#mx = max(yte_r.max(), yte_hat.max())
#plt.plot([mn, mx], [mn, mx])
#plt.title("Regression: True vs Pred (test)")
#plt.xlabel("True"); plt.ylabel("Pred")
#plt.grid(True, alpha=0.25)
#plt.show()

INVALID_VALUE = -999.0

lon_all = df_cls["lon"].values
lat_all = df_cls["lat"].values
lon_s_all , lat_s_all = scale_lonlat_to_m1p1(lon_all, lat_all)
X_all = np.vstack([lon_s_all, lat_s_all])

# h(x)
p_all, _ = forward_pass(X_all, params_c, task="clf")
p_all = p_all.ravel()
cls_all = (p_all >= 0.5)
h = np.full(p_all.shape[0], INVALID_VALUE, dtype=float)
if cls_all.any():
    y_reg_all , _ = forward_pass(X_all[:, cls_all], params_r, task="reg")
    h[cls_all] = y_reg_all.ravel()

lon_u = np.sort(df_cls["lon"].unique())
lat_u = np.sort(df_cls["lat"].unique())
H = h.reshape(len(lat_u), len(lon_u))

H_masked = np.ma.masked_where(H == INVALID_VALUE, H)
cmap = mpl.cm.get_cmap("jet").copy()
cmap.set_bad(color="black") # ä»¤ -999 é¡¯ç¤ºé»‘è‰²

plt.figure(figsize=(6,6))
m = plt.pcolormesh(lon_u, lat_u, H_masked, shading="auto", cmap=cmap)
plt.xlabel("Longitude"); plt.ylabel("Latitude")
plt.title("Piecewise Temperature Map: h(x) (jet colormap)")
cb = plt.colorbar(m, label="Temperature (Â°C)")

# -999 çš„åœ–ä¾‹
invalid_patch = mpatches.Patch(color="black" , label="Invalid (-999)")
plt.legend(handles=[invalid_patch] , loc="upper right")

df_true = (
    df_cls[["lon","lat"]]
    .merge(df_reg[["lon","lat","value"]], on=["lon","lat"], how="left")
)

# çœŸå¯¦æº«åº¦(æ²’æœ‰å€¼çš„è©±è¨­ç‚º-999)
y_true_all = df_true["value"].fillna(INVALID_VALUE).to_numpy(dtype=float)  # shape (N,)

# h(x)çš„é æ¸¬æº«åº¦èˆ‡çœŸå¯¦æº«åº¦æ¯”è¼ƒ(å…¨éƒ¨çš„é»)ï¼Œè¨ˆç®—æº–ç¢ºç‡(èª¤å·®åœ¨Â±1åº¦å…§ç®—æ­£ç¢º)
diff = h - y_true_all
near_mask_all = (np.abs(diff) <= 1.0)
num_correct_all = int(near_mask_all.sum())
total_pts = int(near_mask_all.size)
acc_1deg_all = num_correct_all / total_pts
print(f"[Â±1Â°C accuracy | all points] {num_correct_all} / {total_pts} ({acc_1deg_all:.2%})")

# åªåœ¨æœ‰æ•¸å€¼çš„é»è¨ˆç®—æº–ç¢ºç‡(èª¤å·®åœ¨Â±1åº¦å…§ç®—æ­£ç¢º)
true_valid = (y_true_all != INVALID_VALUE)
near_mask_valid = (np.abs(diff[true_valid]) <= 1.0)
num_correct_valid = int(near_mask_valid.sum())
total_valid = int(true_valid.sum())
acc_1deg_valid = num_correct_valid / total_valid
print(f"[Â±1Â°C accuracy | valid-only] {num_correct_valid} / {total_valid} ({acc_1deg_valid:.2%})")

# æ”¾åœ–å·¦ä¸‹è§’
ax = plt.gca()
ax.text(
    0.01, 0.01,
    f"Â±1Â°C Accuracy (all): {num_correct_all}/{total_pts} ({acc_1deg_all:.1%})"
    + (f"\nÂ±1Â°C Accuracy (valid): {num_correct_valid}/{total_valid} ({acc_1deg_valid:.1%})" if np.isfinite(acc_1deg_valid) else ""),
    transform=ax.transAxes, ha="left", va="bottom",
    bbox=dict(boxstyle="round", fc="white", alpha=0.85, ec="none"),
    fontsize=10)

plt.tight_layout()
plt.show()

# è³‡æ–™åˆ†å¸ƒ
plt.figure(figsize=(6,6))
sc = plt.scatter(df_reg["lon"], df_reg["lat"],
                 c=df_reg["value"], cmap="jet", s=10)
plt.colorbar(sc, label="Temperature (Â°C)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.title("Temperature Distribution (Scatter Plot)")
plt.show()